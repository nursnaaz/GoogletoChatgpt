{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "140abcbd-b59e-4d38-a39e-05a333677789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As the clock struck midnight, a lone rabbit named Rosie hopped into a magical garden, discovering hidden wonders.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model = \"llama3\")\n",
    "response = llm.complete(\"Tell me a story in 20 words!\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "677395d9-2705-4ad5-ba41-1d28ca24c97c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-groq\n",
      "  Downloading llama_index_llms_groq-0.1.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-llms-groq) (0.10.52.post1)\n",
      "Collecting llama-index-llms-openai-like<0.2.0,>=0.1.3 (from llama-index-llms-groq)\n",
      "  Downloading llama_index_llms_openai_like-0.1.3-py3-none-any.whl.metadata (753 bytes)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/homebrew/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (0.6.3)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (2024.2.0)\n",
      "Requirement already satisfied: httpx in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (0.27.0)\n",
      "Requirement already satisfied: llama-cloud<0.0.7,>=0.0.6 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (0.0.6)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/m.alaudeen/Library/Python/3.11/lib/python/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (1.5.8)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (3.8.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (1.25.0)\n",
      "Requirement already satisfied: openai>=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (1.35.7)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (9.5.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (1.16.0)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.1 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-llms-openai-like<0.2.0,>=0.1.3->llama-index-llms-groq) (0.1.25)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-llms-openai-like<0.2.0,>=0.1.3->llama-index-llms-groq) (4.41.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (1.3.1)\n",
      "Requirement already satisfied: pydantic>=1.10 in /opt/homebrew/lib/python3.11/site-packages (from llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (2.5.2)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (3.7.1)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/homebrew/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (3.4)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (0.14.0)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (1.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (2.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/homebrew/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (3.0.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.2.0,>=0.1.3->llama-index-llms-groq) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.2.0,>=0.1.3->llama-index-llms-groq) (0.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.2.0,>=0.1.3->llama-index-llms-groq) (23.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.2.0,>=0.1.3->llama-index-llms-groq) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.2.0,>=0.1.3->llama-index-llms-groq) (0.4.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (3.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/m.alaudeen/Library/Python/3.11/lib/python/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /opt/homebrew/lib/python3.11/site-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (2.14.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/m.alaudeen/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-groq) (1.16.0)\n",
      "Downloading llama_index_llms_groq-0.1.4-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_llms_openai_like-0.1.3-py3-none-any.whl (3.0 kB)\n",
      "Installing collected packages: llama-index-llms-openai-like, llama-index-llms-groq\n",
      "Successfully installed llama-index-llms-groq-0.1.4 llama-index-llms-openai-like-0.1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install llama-index qdrant_client torch transformers \n",
    "#!pip install llama-index-llms-ollama\n",
    "#!pip install llama-index-embeddings-huggingface\n",
    "!pip install llama-index-llms-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f61f7295-9111-41b5-b70e-d409a717c8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75429d44bdc4467ab4e64a18787a987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a856418a7948cdb19b0f081962cfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fb25da0f42489083dad3a5864e76ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215593a2236f4050aeadfe1b41ed844e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380cd526c40241aa8a4fea59c7adf965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21efce2516c1439bb725c2141489189c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ca04e83f294cf58d231b0d5083679f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03f0d6adf6d4d3d829245bfe6691691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f934f85b9a842c5b8a2f4f8706c1e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55ec6cba5824e2283935bba429d49b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101aa818792c4cf8811478c6b13ec24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shopify scaled their MySQL database by employing federation as their initial strategy. They broke up the primary database into smaller MySQL databases, identifying groups of large tables that could exist separately without requiring many joins between them. However, they eventually ran into issues with this approach, including unable to further split the primary database, long schema migrations, and interrupted background jobs. Instead, they pivoted to Vitess as their new scaling strategy.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core import PromptTemplate, Settings\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "\n",
    "def ingest_load(query):\n",
    "    # only load PDFs files\n",
    "    required_exts = [\".pdf\"]\n",
    "\n",
    "    # load documents \n",
    "    loader = SimpleDirectoryReader(\n",
    "                            \"data\", \n",
    "                            required_exts= required_exts\n",
    "                        )\n",
    "\n",
    "    documents = loader.load_data()\n",
    "\n",
    "    # create embeddings using HuggingFace model\n",
    "    embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "    # prompt template\n",
    "    template =  (\n",
    "        \"We have provided context information below. \\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\n---------------------\\n\"\n",
    "        \"Given this information, please answer the question: {query_str}\\n\"\n",
    "        \"If you don't know the answer, please do mention : I don't know !\"\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(template = template)\n",
    "\n",
    "    # define llms\n",
    "    llm = Ollama(model=\"llama3\", request_timeout= 3000)\n",
    "\n",
    "    # setting up llm and output tokens\n",
    "    Settings.llm = llm\n",
    "    Settings.num_output = 250\n",
    "    Settings.embed_model = embed_model\n",
    "\n",
    "    # define index\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "    # define query engine \n",
    "    query_engine = index.as_query_engine()\n",
    "\n",
    "    # update our custom prompt\n",
    "    query_engine.update_prompts(prompt)\n",
    "\n",
    "    # Ask query and get response\n",
    "    response = query_engine.query(query)\n",
    "\n",
    "    print(response)\n",
    "\n",
    "ingest_load(\"How did shopify scale their database processing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "036d20d3-4393-42a7-ac9b-ba1ab08e00df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training process described in the context involves a pretraining stage followed by Supervised Fine Tuning (SFT) and then Reward Modeling and Reinforcement Learning. After these stages, the resulting model is capable of performing well in tasks such as sentiment classification, question answering, chat assistant, etc. The SFT Model generates responses that are scored by human contractors using a reward model, which predicts how well the generated response answers the prompt. This process allows for training a Reinforcement Learning (RLHF) model that can score responses and fine-tune the SFT model to achieve better performance.\n",
      "\n",
      "As a result of this process, the RLHF model is able to achieve high efficiency and performance in tasks such as chat assistance, question answering, and sentiment classification.\n"
     ]
    }
   ],
   "source": [
    "ingest_load(\"How much efficiency and performance improvement they acheived after doing this federtion strategy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd0f3d2b-c753-4012-a663-a6ef16d56041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shopify initially scaled their database processing using federation, where they broke up their primary database into smaller MySQL databases by identifying independent table groups. However, they eventually faced issues with this approach, including being unable to further split the primary database, long schema migrations, and interrupted background jobs. They then pivoted to using Vitess, an open-source sharding solution for MySQL, to overcome these scaling challenges.\n"
     ]
    }
   ],
   "source": [
    "#pip install llama-index-llms-groq\n",
    "from llama_index.llms.groq import Groq\n",
    "#pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core import PromptTemplate, Settings\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "def groq_ingest_load(query):\n",
    "\n",
    "    # only load PDFs files\n",
    "    required_exts = [\".pdf\"]\n",
    "\n",
    "    # load documents \n",
    "    loader = SimpleDirectoryReader(\n",
    "                            \"data\", \n",
    "                            required_exts= required_exts\n",
    "                        )\n",
    "\n",
    "    documents = loader.load_data()\n",
    "\n",
    "    # create embeddings using HuggingFace model\n",
    "    embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "    # prompt template\n",
    "    template =  (\n",
    "        \"We have provided context information below. \\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\n---------------------\\n\"\n",
    "        \"Given this information, please answer the question: {query_str}\\n\"\n",
    "        \"If you don't know the answer, please do mention : I don't know !\"\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(template = template)\n",
    "\n",
    "    # define llms\n",
    "    llm = Groq(model=\"llama3-70b-8192\", api_key= api_key)\n",
    "\n",
    "    # setting up llm and output tokens\n",
    "    Settings.llm = llm\n",
    "    Settings.num_output = 250\n",
    "    Settings.embed_model = embed_model\n",
    "\n",
    "    # define index\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "    # define query engine \n",
    "    query_engine = index.as_query_engine()\n",
    "\n",
    "    # update our custom prompt\n",
    "    query_engine.update_prompts(prompt)\n",
    "\n",
    "    # Ask query and get response\n",
    "    response = query_engine.query(query)\n",
    "\n",
    "    print(response)\n",
    "\n",
    "\n",
    "groq_ingest_load(\"How did shopify scale their database processing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a6b869d-df54-4f25-b1d9-f8884f8fdb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context does not provide a direct answer to the query about the efficiency and performance improvement achieved after a \"federation strategy.\" However, it discusses the training process of ChatGPT, including pretraining, supervised fine-tuning, reward modeling, and reinforcement learning. \n",
      "\n",
      "It mentions that the pretraining stage accounts for 99% of the total compute time needed to train ChatGPT, and it took 21 days of training with 2048 A100 GPUs, costing $5 million USD. The fine-tuning stages, including supervised fine-tuning, reward modeling, and reinforcement learning, are described, but no specific efficiency or performance improvement metrics are provided.\n"
     ]
    }
   ],
   "source": [
    "groq_ingest_load(\"How much efficiency and performance improvement they acheived after doing this federtion strategy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3df395f6-b1ff-4b76-95c2-55c1b560d37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shopify's first strategy to scale MySQL was federation. This involved taking the primary database and breaking it up into smaller MySQL databases. They identified groups of large tables in the primary database that could exist separately - these table groups were independent from each other and didnâ€™t have many queries that required joins between them.\n",
      "The function llama3 took 15.256630182266235 seconds to execute.\n",
      "Shopify initially employed federation as their scaling strategy, breaking up their primary database into smaller MySQL databases by identifying independent table groups. However, they eventually faced issues with this approach, including being unable to further split the primary database, long schema migrations, and interrupted background jobs. They then pivoted to Vitess, an open-source sharding solution for MySQL, to overcome these scaling pains.\n",
      "The function groq llama3 took 5.633556127548218 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from ollama_llama3 import ingest_load\n",
    "from llama3_groq import groq_ingest_load\n",
    "\n",
    "def get_execution_time():\n",
    "    # get the start time \n",
    "    start_time = time.time()\n",
    "    ingest_load(\"How did shopify scale their database processing?\")\n",
    "    end_time = time.time()\n",
    "\n",
    "    # calculate execution time\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"The function llama3 took {execution_time} seconds to execute.\")\n",
    "\n",
    "    # get the start time \n",
    "    start_time1 = time.time()\n",
    "    groq_ingest_load(\"How did shopify scale their database processing?\")\n",
    "    end_time1 = time.time()\n",
    "\n",
    "    # calculate execution time\n",
    "    execution_time1 = end_time1 - start_time1\n",
    "    print(f\"The function groq llama3 took {execution_time1} seconds to execute.\")\n",
    "\n",
    "get_execution_time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91877e1-f4e0-48d6-b570-297771cda77d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
